{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, input_size, output_size=1, hidden_layers=1, neurons=1, init='normal', activation='sigmoid', learning_rate=0.1):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layers = hidden_layers\n",
    "        self.neurons = neurons\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # For convenience, neurons are arranged in row vectors. \n",
    "        # This is contrary to most NN graphs, but it avoids rearranging the weights for the dot product\n",
    "        \n",
    "        def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "            return truncnorm((low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "        \n",
    "        # Create initial guess for weights and biases\n",
    "        self.W = []\n",
    "        if init.lower() == 'ones':\n",
    "            weights_in_hidden = np.ones((input_size,neurons))\n",
    "            weights_hidden_out = np.ones((neurons,output_size))\n",
    "            self.B = [np.zeros((1,neurons)),np.zeros((1,output_size))]\n",
    "        elif init.lower() == 'truncated_normal':\n",
    "            rad = 1 / np.sqrt(input_size)\n",
    "            X = truncated_normal(mean=2,sd=1,low=-rad,upp=rad)\n",
    "            weights_in_hidden = X.rvs((input_size,neurons))\n",
    "            rad = 1 / np.sqrt(neurons)\n",
    "            X = truncated_normal(mean=2,sd=1,low=-rad,upp=rad)\n",
    "            weights_hidden_out = X.rvs((neurons,output_size))\n",
    "            self.B = [np.random.randn(1,neurons),np.random.randn(1,output_size)]\n",
    "        else:\n",
    "            weights_in_hidden = np.random.randn(input_size,neurons)\n",
    "            weights_hidden_out = np.random.randn(neurons,output_size)\n",
    "            self.B = [np.random.randn(1,neurons),np.random.randn(1,output_size)]\n",
    "        \n",
    "        self.W.append(weights_in_hidden)\n",
    "        self.W.append(weights_hidden_out)\n",
    "        \n",
    "        # Initialize cache for neuron values (needed for the error backpropagation step)\n",
    "            # We store values both before and after activation\n",
    "            # neuron_cache contains all neuron values, including input, hidden layers (activated), and output (activated)\n",
    "            # output_cache contains pre activation output from previous layers\n",
    "        self.neuron_cache = []\n",
    "        self.output_cache = []\n",
    "    \n",
    "    def activate(self, x):\n",
    "        if self.activation.lower()=='relu':\n",
    "            return Activation.reLU(x)\n",
    "        elif self.activation.lower()=='softmax':\n",
    "            return Activation.softmax(x)\n",
    "        else:\n",
    "            return Activation.sigmoid(x)\n",
    "    \n",
    "    def deriv_activation(self, x):\n",
    "        if self.activation.lower()=='relu':\n",
    "            return Activation.deriv_reLU(x)\n",
    "        else:\n",
    "            return Activation.deriv_sigmoid(x)\n",
    "        \n",
    "    # Uses previous neuron values (input) to compute the values in the next layer (output)\n",
    "    def feedforward(self, input):\n",
    "        self.neuron_cache = [input]\n",
    "        self.output_cache = []\n",
    "        for layer in range(len(self.W)):\n",
    "            weights = self.W[layer]\n",
    "            bias = self.B[layer]\n",
    "\n",
    "            output = np.dot(input,weights)+bias\n",
    "            self.output_cache.append(output)\n",
    "            neurons = self.activate(output)\n",
    "            self.neuron_cache.append(neurons)\n",
    "            input = neurons\n",
    "        return neurons\n",
    "    \n",
    "    def loss(self, y_true,y_pred):\n",
    "        y_pred = y_pred.reshape(y_true.shape)\n",
    "        return ((y_true - y_pred)**2).mean()\n",
    "    \n",
    "    # Computes the loss between prediction and true training output, then propagates the gradient backwards\n",
    "    def backprop(self, y_true,y_pred, learning_rate):\n",
    "        # loss derivative\n",
    "        dL = -2 * (y_true - y_pred)\n",
    "        dh = 1\n",
    "        for layer in range(len(self.W)-1,-1,-1):\n",
    "            weights = self.W[layer]\n",
    "            bias = self.B[layer]\n",
    "            h = self.neuron_cache[layer]\n",
    "            o = self.output_cache[layer]\n",
    "            \n",
    "            # read dh from previous layer and update dL for next layer\n",
    "            dL = dh * dL\n",
    "            \n",
    "            f = self.deriv_activation(o)\n",
    "            dhdw = np.dot(h.T,f)\n",
    "            \n",
    "            dw = -learning_rate * dL * dhdw\n",
    "            db = -learning_rate * dL * f\n",
    "            \n",
    "            # pass dh to next layer\n",
    "            dh = np.dot(f,weights.T)\n",
    "            \n",
    "            # update weights and biases\n",
    "            self.W[layer] += dw\n",
    "            self.B[layer] += db\n",
    "            print(\"\\tThe weights increment are:\")\n",
    "            print(dw)\n",
    "            print()\n",
    "        pass\n",
    "    \n",
    "    def train(self, data,all_y_true,epochs=5):\n",
    "        learning_rate = self.learning_rate\n",
    "        print('Training the network on %s data points, for %s epochs, with learning rate = %s\\n' %(len(all_y_true),epochs,learning_rate))\n",
    "        Losses = []\n",
    "        for epoch in range(epochs):\n",
    "            for input, y_true in zip(data,all_y_true):\n",
    "                print(\"Training data point is: %s\" %input)\n",
    "                print(\"Expected output is: %s\" %y_true)\n",
    "                input = input.reshape(1,len(input))\n",
    "                y_pred = self.feedforward(input)\n",
    "                print(\"Predicted output is: %s\" %y_pred)\n",
    "                print(\"\\tWeights are:\")\n",
    "                print(self.W)\n",
    "                self.backprop(y_true,y_pred,learning_rate)\n",
    "            pass\n",
    "            # Calculate total loss at the end of each epoch\n",
    "            if epoch % 1 == 0:\n",
    "                predictions = np.apply_along_axis(self.feedforward,1,data)\n",
    "                L = self.loss(all_y_true, predictions)\n",
    "                Losses.append(L)\n",
    "                print(\"Epoch %d loss: %.3f\\n\" % (epoch+1, L))\n",
    "        print()\n",
    "        return Losses\n",
    "\n",
    "    \n",
    "class Activation:\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def deriv_sigmoid(x):\n",
    "        fx = 1 / (1 + np.exp(-x))\n",
    "        return fx * (1 - fx)\n",
    "        \n",
    "    def reLU(x):\n",
    "        return np.maximum(0,x)\n",
    "    def deriv_reLU(x):\n",
    "        x2 = x.reshape(max(x.shape))\n",
    "        y2 = x2*0\n",
    "        for i in range(len(x2)):\n",
    "            if x2[i] != 0:\n",
    "                y2[i] = np.maximum(0,x2[i]) / x2[i]\n",
    "            else:\n",
    "                y2[i] = 0\n",
    "        y = y2.reshape(x.shape)\n",
    "        return y\n",
    "    \n",
    "    def softmax(x):\n",
    "        return np.exp(x) / sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = np.array([\n",
    "  [-2, -1],  # Alice\n",
    "  [25, 6],   # Bob\n",
    "  [17, 4],   # Charlie\n",
    "  [-15, -6], # Diana\n",
    "])\n",
    "all_y_true = np.array([\n",
    "  1, # Alice\n",
    "  0, # Bob\n",
    "  0, # Charlie\n",
    "  1, # Diana\n",
    "])\n",
    "\n",
    "network = Network(2,1,1,2,'ones')\n",
    "L1 = network.train(data,all_y_true)\n",
    "network = Network(2,1,1,2,'normal')\n",
    "L2 = network.train(data,all_y_true)\n",
    "network = Network(2,1,1,2,'truncated_normal')\n",
    "L3 = network.train(data,all_y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(L1,label='Ones')\n",
    "plt.plot(L2,label='Normal')\n",
    "plt.plot(L3,label='truncated normal')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make some predictions\n",
    "emily = np.array([-7, -3]) # 128 pounds, 63 inches\n",
    "frank = np.array([20, 2])  # 155 pounds, 68 inches\n",
    "print(\"Emily: %.3f\" % network.feedforward(emily)) # 0.951 - F\n",
    "print(\"Frank: %.3f\" % network.feedforward(frank)) # 0.039 - M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = np.array([\n",
    "  [-2, -1],  # Alice\n",
    "  [25, 6],   # Bob\n",
    "  [17, 4],   # Charlie\n",
    "  [-15, -6], # Diana\n",
    "])\n",
    "all_y_true = np.array([\n",
    "  1, # Alice\n",
    "  0, # Bob\n",
    "  0, # Charlie\n",
    "  1, # Diana\n",
    "])\n",
    "\n",
    "network = Network(2,1,1,2,_,_,1)\n",
    "L1 = network.train(data,all_y_true)\n",
    "network = Network(2,1,1,2,_,_,0.1)\n",
    "L2 = network.train(data,all_y_true)\n",
    "network = Network(2,1,1,2,_,_,0.01)\n",
    "L3 = network.train(data,all_y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(L1,label='1')\n",
    "plt.plot(L2,label='0.1')\n",
    "plt.plot(L3,label='0.01')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Make some predictions\n",
    "emily = np.array([-7, -3]) # 128 pounds, 63 inches\n",
    "frank = np.array([20, 2])  # 155 pounds, 68 inches\n",
    "print(\"Emily: %.3f\" % network.feedforward(emily)) # 0.951 - F\n",
    "print(\"Frank: %.3f\" % network.feedforward(frank)) # 0.039 - M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the network on 4 data points, for 5 epochs, with learning rate = 0.1\n",
      "\n",
      "Training data point is: [-2 -1]\n",
      "Expected output is: 1\n",
      "Predicted output is: [[13.90215459]]\n",
      "\tWeights are:\n",
      "[array([[-0.23179511, -2.3163552 ],\n",
      "       [-1.24134263,  0.47639697]]), array([[-0.23173698],\n",
      "       [ 2.87462014]])]\n",
      "\tThe weights increment are:\n",
      "[[ -0.20899093]\n",
      " [-13.37962626]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[-1.19596253 14.83551735]\n",
      " [-0.59798127  7.41775868]]\n",
      "\n",
      "Training data point is: [25  6]\n",
      "Expected output is: 0\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "Training data point is: [17  4]\n",
      "Expected output is: 0\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "Training data point is: [-15  -6]\n",
      "Expected output is: 1\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[-0.  0.]\n",
      " [-0.  0.]]\n",
      "\n",
      "Epoch 1 loss: 0.500\n",
      "\n",
      "Training data point is: [-2 -1]\n",
      "Expected output is: 1\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[-0.  0.]\n",
      " [-0.  0.]]\n",
      "\n",
      "Training data point is: [25  6]\n",
      "Expected output is: 0\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "Training data point is: [17  4]\n",
      "Expected output is: 0\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "Training data point is: [-15  -6]\n",
      "Expected output is: 1\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[-0.  0.]\n",
      " [-0.  0.]]\n",
      "\n",
      "Epoch 2 loss: 0.500\n",
      "\n",
      "Training data point is: [-2 -1]\n",
      "Expected output is: 1\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[-0.  0.]\n",
      " [-0.  0.]]\n",
      "\n",
      "Training data point is: [25  6]\n",
      "Expected output is: 0\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "Training data point is: [17  4]\n",
      "Expected output is: 0\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "Training data point is: [-15  -6]\n",
      "Expected output is: 1\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[-0.  0.]\n",
      " [-0.  0.]]\n",
      "\n",
      "Epoch 3 loss: 0.500\n",
      "\n",
      "Training data point is: [-2 -1]\n",
      "Expected output is: 1\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[-0.  0.]\n",
      " [-0.  0.]]\n",
      "\n",
      "Training data point is: [25  6]\n",
      "Expected output is: 0\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "Training data point is: [17  4]\n",
      "Expected output is: 0\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "Training data point is: [-15  -6]\n",
      "Expected output is: 1\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[-0.  0.]\n",
      " [-0.  0.]]\n",
      "\n",
      "Epoch 4 loss: 0.500\n",
      "\n",
      "Training data point is: [-2 -1]\n",
      "Expected output is: 1\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[-0.  0.]\n",
      " [-0.  0.]]\n",
      "\n",
      "Training data point is: [25  6]\n",
      "Expected output is: 0\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "Training data point is: [17  4]\n",
      "Expected output is: 0\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "\n",
      "Training data point is: [-15  -6]\n",
      "Expected output is: 1\n",
      "Predicted output is: [[0.]]\n",
      "\tWeights are:\n",
      "[array([[-1.42775764, 12.51916215],\n",
      "       [-1.8393239 ,  7.89415565]]), array([[ -0.4407279 ],\n",
      "       [-10.50500613]])]\n",
      "\tThe weights increment are:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "\tThe weights increment are:\n",
      "[[-0.  0.]\n",
      " [-0.  0.]]\n",
      "\n",
      "Epoch 5 loss: 0.500\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5, 0.5, 0.5, 0.5, 0.5]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([\n",
    "  [-2, -1],  # Alice\n",
    "  [25, 6],   # Bob\n",
    "  [17, 4],   # Charlie\n",
    "  [-15, -6], # Diana\n",
    "])\n",
    "all_y_true = np.array([\n",
    "  1, # Alice\n",
    "  0, # Bob\n",
    "  0, # Charlie\n",
    "  1, # Diana\n",
    "])\n",
    "\n",
    "network = Network(2,1,1,2,_,'reLU')\n",
    "network.train(data,all_y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = np.array([\n",
    "  [-2, -1],  # Alice\n",
    "  [25, 6],   # Bob\n",
    "  [17, 4],   # Charlie\n",
    "  [-15, -6], # Diana\n",
    "])\n",
    "all_y_true = np.array([\n",
    "  1, # Alice\n",
    "  0, # Bob\n",
    "  0, # Charlie\n",
    "  1, # Diana\n",
    "])\n",
    "\n",
    "network = Network(2,1,1,2,_,'sigmoid',1)\n",
    "L1 = network.train(data,all_y_true)\n",
    "network = Network(2,1,1,2,_,'reLU',1)\n",
    "L2 = network.train(data,all_y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(L1,label='sigmoid')\n",
    "plt.plot(L2,label='reLU')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "axis = np.linspace(-5,5,101)\n",
    "print(axis)\n",
    "ax[0].plot(axis,Activation.reLU(axis))\n",
    "ax[1].plot(axis,Activation.deriv_reLU(axis))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
